{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://aclanthology.org/2020.acl-main.90.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/@waqarhaider1292/chatbot-by-huggingface-document-based-question-answering-c4fd94ec35db\n",
    "\n",
    "https://medium.com/@lokaregns/question-answering-with-hugging-face-transformers-a-beginners-guide-487ae1a91b9a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import pipeline\n",
    "\n",
    "# # Load the question answering pipeline\n",
    "# model_name = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
    "# qa_pipeline = pipeline(\"question-answering\", model=model_name, tokenizer=model_name)\n",
    "\n",
    "# # Initialize conversation history dictionary\n",
    "# conversation_history = {\n",
    "#     \"context\": None,\n",
    "#     \"questions\": [],\n",
    "#     \"answers\": []\n",
    "# }\n",
    "\n",
    "# def answer_question(context, question):\n",
    "#     # Use the QA pipeline to answer the question based on the context\n",
    "#     try:\n",
    "#         result = qa_pipeline(question=question, context=context)\n",
    "#         return result\n",
    "#     except Exception as e:\n",
    "#         print(\"Error occurred while answering the question:\", e)\n",
    "#         return \"Sorry, I couldn't find an answer to that question.\"\n",
    "\n",
    "# def detect_follow_up(question):\n",
    "#     # Simple algorithm to detect follow-up questions\n",
    "#     keywords = [\"next\", \"then\", \"after\", \"following\"]\n",
    "#     for keyword in keywords:\n",
    "#         if keyword in question.lower():\n",
    "#             return True\n",
    "#     return False\n",
    "\n",
    "# def add_to_conversation_history(context, question, answer):\n",
    "#     # Add the current context, question, and answer to conversation history\n",
    "#     conversation_history[\"context\"] = context\n",
    "#     conversation_history[\"questions\"].append(question)\n",
    "#     conversation_history[\"answers\"].append(answer)\n",
    "\n",
    "# def answer_with_history(question):\n",
    "#     # Check if the current question is a follow-up question\n",
    "#     is_follow_up = detect_follow_up(question)\n",
    "    \n",
    "#     if is_follow_up:\n",
    "#         # If it's a follow-up question, use the context and previous answers to answer\n",
    "#         context = conversation_history[\"context\"]\n",
    "#         questions = conversation_history[\"questions\"]\n",
    "#         answers = conversation_history[\"answers\"]\n",
    "#         for i, prev_question in enumerate(reversed(questions)):\n",
    "#             if detect_follow_up(prev_question):\n",
    "#                 continue\n",
    "#             context = context.replace(answers[-(i+1)], \"\")  # Remove previous answer from context\n",
    "#             break\n",
    "#         return answer_question(context, question)\n",
    "#     else:\n",
    "#         # If it's not a follow-up question, answer it directly\n",
    "#         return answer_question(conversation_history[\"context\"], question)\n",
    "\n",
    "# # Example context\n",
    "# context = \"The Apollo program, also known as Project Apollo, was the third United States human spaceflight program carried out by the National Aeronautics and Space Administration (NASA), which accomplished landing the first humans on the Moon from 1969 to 1972. First conceived during the Eisenhower administration as a three-man spacecraft to follow the one-man Project Mercury which put the first Americans in space, Apollo was later dedicated to President John F. Kennedy's national goal for the 1960s of 'landing a man on the Moon and returning him safely to the Earth'.\"\n",
    "\n",
    "# # Example questions\n",
    "# questions = [\n",
    "#     \"What was the Apollo program?\",\n",
    "#     \"after Who carried out the Apollo program?\",\n",
    "#     \"When did the Apollo program take place?\",\n",
    "#     \"What happened after the Apollo program?\",\n",
    "#     \"How was the Apollo program followed?\"\n",
    "# ]\n",
    "\n",
    "# # Answer the questions with conversation history tracking\n",
    "# for question in questions:\n",
    "#     answer = answer_with_history(question)\n",
    "#     add_to_conversation_history(context, question, answer)\n",
    "#     print(\"Question:\", question)\n",
    "#     print(\"Answer:\", answer)\n",
    "#     print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import PySimpleGUI as sg\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load the question answering pipeline\n",
    "model_name = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
    "qa_pipeline = pipeline(\"question-answering\", model=model_name, tokenizer=model_name)\n",
    "\n",
    "# Initialize conversation history dictionary\n",
    "conversation_history = {\n",
    "    \"context\": None,\n",
    "    \"questions\": [],\n",
    "    \"answers\": []\n",
    "}\n",
    "\n",
    "def answer_question(context, question):\n",
    "    try:\n",
    "        if len(conversation_history['questions']) != 0:\n",
    "            concatenated_text = \"\"\n",
    "            for history_question, answer in zip(conversation_history[\"questions\"], conversation_history[\"answers\"]):\n",
    "                concatenated_text += f\"{history_question} {answer}. \"\n",
    "            \n",
    "            question = concatenated_text + question\n",
    "\n",
    "\n",
    "        result = qa_pipeline(question=question, context=context)\n",
    "        return question, result['answer'], result['score']\n",
    "    except Exception as e:\n",
    "        print(\"Error occurred while answering the question:\", e)\n",
    "        return \"Sorry, I couldn't find an answer to that question.\"\n",
    "\n",
    "\n",
    "\n",
    "def add_to_conversation_history(context, question, answer):\n",
    "    # Add the current context, question, and answer to conversation history\n",
    "    conversation_history[\"context\"] = context\n",
    "    conversation_history[\"questions\"].append(question)\n",
    "    conversation_history[\"answers\"].append(answer)\n",
    "\n",
    "# def answer_with_history(question):\n",
    "\n",
    "#         # If it's a follow-up question, use the context and previous answers to answer\n",
    "#         context = conversation_history[\"context\"]\n",
    "#         questions = conversation_history[\"questions\"]\n",
    "#         answers = conversation_history[\"answers\"]\n",
    "#         return answer_question(context, question)\n",
    "#         # If it's not a follow-up question, answer it directly\n",
    "\n",
    "# Define GUI layout\n",
    "layout = [\n",
    "    [sg.Text(\"Question-Answering System\")],\n",
    "    [sg.Text(\"Context:\"), sg.Multiline(key='-CONTEXT-', size=(60, 10))],\n",
    "    [sg.Multiline(size=(60, 10), key='-OUTPUT-', disabled=True)],\n",
    "    [sg.InputText(key='-INPUT-')],\n",
    "    [sg.Button('Ask'), sg.Button('Exit')],\n",
    "]\n",
    "\n",
    "# Create the GUI window\n",
    "window = sg.Window('QA System', layout)\n",
    "\n",
    "# Main event loop\n",
    "while True:\n",
    "    event, values = window.read()\n",
    "\n",
    "    if event == sg.WINDOW_CLOSED or event == 'Exit':\n",
    "        break\n",
    "\n",
    "    user_input = values['-INPUT-']\n",
    "    context = values['-CONTEXT-']\n",
    "\n",
    "    if user_input:\n",
    "        question = user_input\n",
    "\n",
    "        # Get response to the user's question\n",
    "        concat, answer, score = answer_question(context, question)\n",
    "        add_to_conversation_history(context, question, answer)\n",
    "\n",
    "\n",
    "        # Display the answer in the GUI window\n",
    "        window['-OUTPUT-'].print(\"Question: \" + concat)\n",
    "        window['-OUTPUT-'].print(\"Answer: \" + answer)\n",
    "        window['-OUTPUT-'].print(\"Score: \" + str(score))\n",
    "        window['-OUTPUT-'].print(\"\")\n",
    "\n",
    "window.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.12360134012040346\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load English language model\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Define two example sentences\n",
    "sentence1 = \"who is b? Tong.\"\n",
    "sentence2 = \"cat\"\n",
    "\n",
    "# Process the sentences with spaCy\n",
    "doc1 = nlp(sentence1)\n",
    "doc2 = nlp(sentence2)\n",
    "\n",
    "# Calculate cosine similarity between the document vectors\n",
    "similarity = doc1.similarity(doc2)\n",
    "print(\"Cosine Similarity:\", similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\datasets\\load.py:756: FutureWarning: The repository for bleu contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/bleu/bleu.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Got a string but expected a list instead: 'there'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m generated_tokens \u001b[38;5;241m=\u001b[39m word_tokenize(generated\u001b[38;5;241m.\u001b[39mlower())\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Calculate BLEU score\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m bleu_score \u001b[38;5;241m=\u001b[39m \u001b[43mbleu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mreference_tokens\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreferences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mgenerated_tokens\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBLEU Score:\u001b[39m\u001b[38;5;124m\"\u001b[39m, bleu_score)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\datasets\\metric.py:444\u001b[0m, in \u001b[0;36mMetric.compute\u001b[1;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[0;32m    441\u001b[0m compute_kwargs \u001b[38;5;241m=\u001b[39m {k: kwargs[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures}\n\u001b[0;32m    443\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m--> 444\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    445\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_finalize()\n\u001b[0;32m    447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\datasets\\metric.py:496\u001b[0m, in \u001b[0;36mMetric.add_batch\u001b[1;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[0;32m    494\u001b[0m batch \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredictions\u001b[39m\u001b[38;5;124m\"\u001b[39m: predictions, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreferences\u001b[39m\u001b[38;5;124m\"\u001b[39m: references, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[0;32m    495\u001b[0m batch \u001b[38;5;241m=\u001b[39m {intput_name: batch[intput_name] \u001b[38;5;28;01mfor\u001b[39;00m intput_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures}\n\u001b[1;32m--> 496\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    498\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_writer()\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\datasets\\features\\features.py:1923\u001b[0m, in \u001b[0;36mFeatures.encode_batch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m   1921\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, column \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   1922\u001b[0m     column \u001b[38;5;241m=\u001b[39m cast_to_python_objects(column)\n\u001b[1;32m-> 1923\u001b[0m     encoded_batch[key] \u001b[38;5;241m=\u001b[39m [\u001b[43mencode_nested_example\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m column]\n\u001b[0;32m   1924\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m encoded_batch\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\datasets\\features\\features.py:1295\u001b[0m, in \u001b[0;36mencode_nested_example\u001b[1;34m(schema, obj, level)\u001b[0m\n\u001b[0;32m   1290\u001b[0m             \u001b[38;5;66;03m# be careful when comparing tensors here\u001b[39;00m\n\u001b[0;32m   1291\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1292\u001b[0m                 \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(first_elmt, \u001b[38;5;28mlist\u001b[39m)\n\u001b[0;32m   1293\u001b[0m                 \u001b[38;5;129;01mor\u001b[39;00m encode_nested_example(schema\u001b[38;5;241m.\u001b[39mfeature, first_elmt, level\u001b[38;5;241m=\u001b[39mlevel \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m!=\u001b[39m first_elmt\n\u001b[0;32m   1294\u001b[0m             ):\n\u001b[1;32m-> 1295\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mencode_nested_example\u001b[49m\u001b[43m(\u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m obj]\n\u001b[0;32m   1296\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(obj)\n\u001b[0;32m   1297\u001b[0m \u001b[38;5;66;03m# Object with special encoding:\u001b[39;00m\n\u001b[0;32m   1298\u001b[0m \u001b[38;5;66;03m# ClassLabel will convert from string to int, TranslationVariableLanguages does some checks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\datasets\\features\\features.py:1284\u001b[0m, in \u001b[0;36mencode_nested_example\u001b[1;34m(schema, obj, level)\u001b[0m\n\u001b[0;32m   1282\u001b[0m \u001b[38;5;66;03m# schema.feature is not a dict\u001b[39;00m\n\u001b[0;32m   1283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, \u001b[38;5;28mstr\u001b[39m):  \u001b[38;5;66;03m# don't interpret a string as a list\u001b[39;00m\n\u001b[1;32m-> 1284\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot a string but expected a list instead: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobj\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1285\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1286\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(obj) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[1;31mValueError\u001b[0m: Got a string but expected a list instead: 'there'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# from nltk.translate.bleu_score import sentence_bleu\n",
    "# from datasets import load_metric\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# bleu = load_metric(\"bleu\")\n",
    "\n",
    "# # Reference answer\n",
    "# reference = \"The cat is on the mat.\"\n",
    "\n",
    "# # Generated answer\n",
    "# generated = \"There is a cat on the mat.\"\n",
    "\n",
    "# # Tokenize reference and generated answers\n",
    "# reference_tokens = word_tokenize(reference.lower())\n",
    "# generated_tokens = word_tokenize(generated.lower())\n",
    "\n",
    "# # Calculate BLEU score\n",
    "# bleu_score = bleu.compute(predictions=[reference_tokens], references=[generated_tokens])\n",
    "# print(\"BLEU Score:\", bleu_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
